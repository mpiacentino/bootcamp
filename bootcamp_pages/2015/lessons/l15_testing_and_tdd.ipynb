{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 15: Testing and test-driven development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This tutorial was generated from a Jupyter notebook.  You can download the notebook [here](l15_testing_and_tdd.ipynb).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We just need pytest for this lesson\n",
    "import pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test-driven development**, or **TDD**, is a paradigm for dveloping software.  The idea is that a programmer thinks about a design specification for a bit of code, usually a function.  I.e., she lays out what the input and output should be.  She then writes a test (that will fail) for the bit of code.  She then writes or updates the code to pass the test.  She does this **incrementally** as she builds her code.  Let's try this by example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of TDD\n",
    "We will write a function that computes the number of negatively charged residues in a protein.  In other words, we count up the number of glutamate (`E`) and aspartate (`D`) residues.\n",
    "\n",
    "We'll call the function `n_neg()`, and will just make an empty function for now as a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_neg(seq):\n",
    "    \"\"\"Number of negative residues a protein sequence\"\"\"\n",
    "\n",
    "    # Do nothing for now\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll write a very simple test.  It is just a conditional expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neg('E') == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We failed the test!  But before we focus on the test failure, let's think about what we just did.  We defined the prototype for the function.  We know we want it to take in a sequence (a string) and return an integer.  So, in building the test, we have designed the interface for the function.\n",
    "\n",
    "Back to the test failure.  We now have a test we would like out function to pass, and we will now revisit the function to write it so that it will pass the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_neg(seq):\n",
    "    \"\"\"Number of negative residues a protein sequence\"\"\"\n",
    "\n",
    "    # Count E's and D's, since these are the negative residues\n",
    "    return seq.count('E') + seq.count('D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try out test again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neg('E') == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurray!  We passed our first test.  Now, lets write some more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(n_neg('E') == 1)\n",
    "print(n_neg('D') == 1)\n",
    "print(n_neg('') == 0)\n",
    "print(n_neg('ACKLWTTAE') == 1)\n",
    "print(n_neg('DDDDEEEE') == 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function appears to be working well.  But let's think carefully about how we could break it.  What if we had lowercase letters?  I.e., what would we want \n",
    "\n",
    "    n_neg('acklwttae')\n",
    "\n",
    "to return?  Do we allow lower case?  This is an example where coming up with tests is how we define the interface.  We weren't done designing it at the first pass!\n",
    "\n",
    "Let's say we want to allow lower case symbols.  So, before we mess with our function, let's write a test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neg('acklwttae') == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We failed, as expected.  Now, back to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_neg(seq):\n",
    "    \"\"\"Number of negative residues a protein sequence\"\"\"\n",
    "\n",
    "    # Convert sequence to upper case\n",
    "    seq = seq.upper()\n",
    "    \n",
    "    # Count E's and D's, since these are the negative residues\n",
    "    return seq.count('E') + seq.count('D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to run ALL of our tests again.  We have to make sure *everything* passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(n_neg('E') == 1)\n",
    "print(n_neg('D') == 1)\n",
    "print(n_neg('') == 0)\n",
    "print(n_neg('ACKLWTTAE') == 1)\n",
    "print(n_neg('DDDDEEEE') == 8)\n",
    "print(n_neg('acklwttae') == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!  This works now.\n",
    "\n",
    "You can see how the cycle proceeds.  Right now, we might be happy with our function, but as we use it in whatever context we are working in, use cases we have not thought of might creep up.  Everything that happens, or there is a bug you find, *write another test that covers it*.  Importantly, *any time* you update your code, you need to run *all* of your tests!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The **`assert`** statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, we used a bunch of print statements to check our tests.  Conveniently, Python have a built-in way to do your tests using the **`assert`** keyword.  For example, our first test using **`assert`** is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert n_neg('E') == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ran without issue.  Now, let's try asserting something we know will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-188264dd8bc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mn_neg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'E'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert n_neg('E') == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an `AssertionError`, indicating that our assertion failed.  We can even append the **`assert`** statement with a comment describing the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Failed on sequence of length 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b6ed3249ba8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mn_neg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'E'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Failed on sequence of length 1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Failed on sequence of length 1"
     ]
    }
   ],
   "source": [
    "assert n_neg('E') == 2, 'Failed on sequence of length 1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we see the basic syntax of **`assert`** statements.  After **`assert`**, we have a conditional expression that evaluates to `True` or `False`.  If it evaluates `False`, an `AssertionError` is raised, meaning that the test was failed.  Optionally, the conditional expression can be followed with a comma and a string that describes how it failed.  So, we could write all of our tests together as a series of assertions.  Actually, it would be best to write a *function* that does the testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_n_neg():\n",
    "    \"\"\"Perform unit tests on n_neg.\"\"\"\n",
    "\n",
    "    assert n_neg('E') == 1\n",
    "    assert n_neg('D') == 1\n",
    "    assert n_neg('') == 0\n",
    "    assert n_neg('ACKLWTTAE') == 1\n",
    "    assert n_neg('DDDDEEEE') == 8\n",
    "    assert n_neg('acklwttae') == 1\n",
    "\n",
    "\n",
    "# Run all the tests\n",
    "test_n_neg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent!  Everything passed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on assertions vs raising exceptions\n",
    "It is important to draw the distinction between assertions and raising exceptions in your code.  \n",
    "\n",
    "* You should raise exceptions when you are checking inputs to your function.  I.e., you are checking to make sure the user is using the function properly.\n",
    "* You should use assertions to make sure the function operates as expected for given input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `pytest` module\n",
    "The `pytest` (a.k.a. `py.test`) module comes with a standard Anaconda installation and is useful tool for automating your testing.  It gives detailed feedback on your tests.  You can read its documentation [here](http://pytest.org).\n",
    "\n",
    "The `unittest` module from the standard library and `nose` are two other major testing packages for Python.  All three are in common usage.  We use `pytest` here because I think it is the easiest to use and understand.\n",
    "\n",
    "To explore the first feature we'll consider of `pytest`, we'll consider another aspect of our `n_neg()` function that we want to function properly.  Specifically, we want a `RuntimeError` if an invalid sequence is entered.  Again, in designing our test, we need to think about what constitutes an invalid sequence.  We'll only allow the 20 symbols for the residues that we used in the [last lesson](l13_exceptions_and_error_handling.html).  So, we adjust our test function accordingly.  We cannot use the **`assert`** statement to check for proper error handling, so we use the `pytest.raises()` function.  This function takes as its first argument the type of exception expected, and a string containing the code to be run to give the exception.  Note that I used double quotes for the string so I could use single quotes for the string arguments to the `n_neg()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Failed",
     "evalue": "DID NOT RAISE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailed\u001b[0m                                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1d47d1f8f339>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpytest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraises\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRuntimeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_neg('Z')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Justin/anaconda/lib/python3.4/site-packages/_pytest/python.py\u001b[0m in \u001b[0;36mraises\u001b[0;34m(ExpectedException, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mExpectedException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExceptionInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m     \u001b[0mpytest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DID NOT RAISE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mRaisesContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Justin/anaconda/lib/python3.4/site-packages/_pytest/runner.py\u001b[0m in \u001b[0;36mfail\u001b[0;34m(msg, pytrace)\u001b[0m\n\u001b[1;32m    476\u001b[0m     \"\"\"\n\u001b[1;32m    477\u001b[0m     \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFailed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpytrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m \u001b[0mfail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mException\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFailed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailed\u001b[0m: DID NOT RAISE"
     ]
    }
   ],
   "source": [
    "pytest.raises(RuntimeError, \"n_neg('Z')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this means we have to update our function again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_neg(seq):\n",
    "    \"\"\"Number of negative residues a protein sequence\"\"\"\n",
    "    \n",
    "    # Convert sequence to upper case\n",
    "    seq = seq.upper()\n",
    "    \n",
    "    # Check for a valid sequence\n",
    "    for aa in seq:\n",
    "        if aa not in 'ACDEFGHIKLMNPQRSTVWY':\n",
    "            raise RuntimeError(aa + ' is not a valid amino acid.')\n",
    "    \n",
    "    # Count E's and D's, since these are the negative residues\n",
    "    return seq.count('E') + seq.count('D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should work, since it now checks for valid sequences.  We should now include exception handling to our test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_n_neg():\n",
    "    \"\"\"Perform unit tests on n_neg.\"\"\"\n",
    "\n",
    "    assert n_neg('E') == 1\n",
    "    assert n_neg('D') == 1\n",
    "    assert n_neg('') == 0\n",
    "    assert n_neg('ACKLWTTAE') == 1\n",
    "    assert n_neg('DDDDEEEE') == 8\n",
    "    assert n_neg('acklwttae') == 1\n",
    "\n",
    "    pytest.raises(RuntimeError, \"n_neg('Z')\")\n",
    "    pytest.raises(RuntimeError, \"n_neg('z')\")\n",
    "    pytest.raises(RuntimeError, \"n_neg('KAACABAYABADDLKPPSD')\")\n",
    "\n",
    "# Run all the tests\n",
    "test_n_neg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It passes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `pytest` on your software package\n",
    "`pytest` will automatically do your tests for you.  In the simplest implementation, you simply need to do the following.\n",
    "\n",
    "1. For each function `fun` you want to test, write a function called `test_fun` that has all of your unit tests with your **`assert`** statements and checks for RuntimeErrors.\n",
    "2. Put all these tests in a directory called `tests`.  The `tests` directory should be in the directory containing your code.\n",
    "3. Simply `cd` into the directory with your code and enter `py.test` at the command line.  `pytest` will then take over and automatically run all of your unit tests and give you reports.\n",
    "\n",
    "You will do this in tonight's exercises.  \n",
    "\n",
    "There is also a wealth of other testing resources and strategies.  We have only touched on the basics here.  You might want to read the [`pytest` documentation](http://pytest.org/latest/contents.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principles of TDD\n",
    "\n",
    "Finally, we close with a summary of the basic principles of test-driven development.\n",
    "\n",
    "1. Build your software out of **small functions** that do **one specific thing**.\n",
    "2. Build unit tests for all of your functions.\n",
    "3. Whenever you make any enhancements of adjustments to your code, write tests for it.\n",
    "4. Whenever you encounter and squash a bug, write tests for it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
